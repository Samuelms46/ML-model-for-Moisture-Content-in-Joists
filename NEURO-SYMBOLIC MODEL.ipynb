{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-23T14:41:44.362442Z",
     "iopub.status.busy": "2025-02-23T14:41:44.362111Z",
     "iopub.status.idle": "2025-02-23T14:41:44.368072Z",
     "shell.execute_reply": "2025-02-23T14:41:44.366962Z",
     "shell.execute_reply.started": "2025-02-23T14:41:44.362415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"cleaned4_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Define feature and target columns\n",
    "features = [\n",
    "    \"BN1-MCheek\", \"BN1-MJoistUp\", \"BN1-MJoistLo\", \"BN1-RPocket\", \"BN1-RPocket-C\", \"BN1-RPocket-DP\", \"BN1-TPocket\", \"BN1-TCheek\", \n",
    "    \"BN2-MCheek\", \"BN2-MJoistUp\", \"BN2-MJoistLo\", \"BN2-RPocket\", \"BN2-RPocket-C\", \"BN2-RPocket-DP\", \"BN2-TPocket\", \"BN2-TCheek\", \n",
    "    \"BN3-MCheek\", \"BN3-MJoistUp\", \"BN3-MJoistLo\", \"BN3-RPocket\", \"BN3-RPocket-C\", \"BN3-RPocket-DP\", \"BN3-TPocket\", \"BN3-TCheek\",\n",
    "    \"BE2-MCheek\", \"BE2-MJoistUp\", \"BE2-MJoistLo\", \"BE2-RPocket\", \"BE2-RPocket-C\", \"BE2-RPocket-DP\", \"BE2-TPocket\", \"BE2-TCheek\",\n",
    "    \"BS-MCheek\", \"BS-MJoistLeft\", \"BS-MJoistRight\", \"BS-RPocket\", \"BS-RPocket-C\", \"BS-RPocket-DP\", \"BS-TPocket\", \"BS-TCheek\",\n",
    "    \"BW1-MCheek\", \"BW1-MJoistUp\", \"BW1-MJoistLo\", \"BW1-RPocket\", \"BW1-RPocket-C\", \"BW1-RPocket-DP\", \"BW1-TPocket\", \"BW1-TCheek\",\n",
    "    \"BW2-MCheek\", \"BW2-MJoistUp\", \"BW2-MJoistLo\", \"BW2-RPocket\", \"BW2-RPocket-C\", \"BW2-RPocket-DP\", \"BW2-TPocket\", \"BW2-TCheek\"\n",
    "]\n",
    "\n",
    "targets = {\n",
    "    \"BSMT-N1\": [\"BN1-MCheek-MC\", \"BN1-MJoistUp-MC\", \"BN1-MJoistLo-MC\"],\n",
    "    \"BSMT-N2\": [\"BN2-MCheek-MC\", \"BN2-MJoistUp-MC\", \"BN2-MJoistLo-MC\"],\n",
    "    \"BSMT-N3\": [\"BN3-MCheek-MC\", \"BN3-MJoistUp-MC\", \"BN3-MJoistLo-MC\"],\n",
    "    \"BSMT-E2\": [\"BE2-MCheek-MC\", \"BE2-MJoistUp-MC\", \"BE2-MJoistLo-MC\"],\n",
    "    \"BSMT-S\": [\"BS-MCheek-MC\", \"BS-MJoistLeft-MC\", \"BS-MJoistRight-MC\"]\n",
    "}\n",
    "\n",
    "target_columns = [col for cols in targets.values() for col in cols]\n",
    "\n",
    "df = df[features + target_columns].dropna()\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target_columns], test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features list:\", features)\n",
    "num_features = X_train_scaled.shape[1]  # Ensure correct input size\n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "# Define neural network model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(num_features,)),\n",
    "    layers.Dense(64, activation='relu'), \n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(len(target_columns), activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Train Random Forest Model\n",
    "symbolic_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "symbolic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nn_predictions = model.predict(X_test_scaled)\n",
    "symbolic_predictions = symbolic_model.predict(X_test)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - MSE: {mse:.4f}, MAE: {mae:.4f}, R2 Score: {r2:.4f}\")\n",
    "    return mse, mae, r2\n",
    "\n",
    "nn_metrics = evaluate_model(y_test, nn_predictions, \"Neural Network\")\n",
    "symbolic_metrics = evaluate_model(y_test, symbolic_predictions, \"Random Forest\")\n",
    "\n",
    "# Hybrid Model (Averaging Predictions)\n",
    "final_predictions = (nn_predictions + symbolic_predictions) / 2\n",
    "final_metrics = evaluate_model(y_test, final_predictions, \"Hybrid Model\")\n",
    "\n",
    "# Store results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Neural Network\", \"Random Forest\", \"Hybrid Model\"],\n",
    "    \"MSE\": [nn_metrics[0], symbolic_metrics[0], final_metrics[0]],\n",
    "    \"MAE\": [nn_metrics[1], symbolic_metrics[1], final_metrics[1]],\n",
    "    \"R2 Score\": [nn_metrics[2], symbolic_metrics[2], final_metrics[2]]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save models\n",
    "model.save(\"neural_model.h5\")\n",
    "joblib.dump(symbolic_model, \"symbolic_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
